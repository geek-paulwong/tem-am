<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>OpenAI WebRTC Voice Agent</title>
  <style>
    /* Mobile-friendly layout */
    body {
      font-family: Arial, sans-serif;
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: flex-start;
      min-height: 100vh;
      margin: 0;
      padding: 20px;
      background-color: #f4f4f4;
      color: #333;
    }

    h1 {
      font-size: 1.8rem;
      text-align: center;
      margin-bottom: 30px;
    }

    #startBtn {
      padding: 15px 30px;
      font-size: 1.2rem;
      border: none;
      border-radius: 10px;
      background-color: #007bff;
      color: white;
      cursor: pointer;
      transition: background-color 0.2s;
      margin-bottom: 20px;
    }

    #startBtn:active {
      background-color: #0056b3;
    }

    audio {
      margin-top: 20px;
      width: 100%;
      max-width: 500px;
    }

    @media (max-width: 480px) {
      #startBtn {
        width: 100%;
        font-size: 1.1rem;
      }

      h1 {
        font-size: 1.5rem;
      }
    }
  </style>
</head>
<body>
  <h1>WebRTC Voice Agent</h1>
  <button id="startBtn">Start</button>
  <button id="stopBtn">Stop & Send</button>

  <div id="speakingIndicator" style="
    margin-top: 20px;
    padding: 10px 20px;
    border-radius: 10px;
    background-color: #ccc;
    color: #fff;
    font-weight: bold;
    display: none;
    text-align: center;
    width: 150px;
  ">Speaking...</div>

  <div id="debugPanel" style="
    position: fixed;
    bottom: 10px;
    left: 10px;
    right: 10px;
    max-height: 200px;
    overflow-y: auto;
    background: rgba(0,0,0,0.7);
    color: #fff;
    font-family: monospace;
    font-size: 12px;
    padding: 10px;
    border-radius: 8px;
    z-index: 1000;
  "></div>

  <script>
    function debugLog(...args) {
      console.log(...args);
      const panel = document.getElementById("debugPanel");
      if (panel) {
        const msg = args.map(a => (typeof a === "object" ? JSON.stringify(a) : a)).join(" ");
        const line = document.createElement("div");
        line.textContent = `[DEBUG] ${msg}`;
        panel.appendChild(line);
        panel.scrollTop = panel.scrollHeight;
      }
    }

    let micStream;
    let pc;         // PeerConnection, needs to be accessible in stopAndSend()
    let clientSecret; // client secret from backend

    async function start() {
      debugLog("Starting WebRTC session...");
      try {
        // 1ï¸âƒ£ Request session from backend (gets client_secret only)
        const session = await fetch("/session-webrtc", { method: "POST" }).then(r => r.json());
        debugLog("Received session from backend:", session);
        if (!session.client_secret?.value) {
          console.error("[ERROR] Missing client_secret in session response:", session);
          return;
        }
        clientSecret = session.client_secret.value;

        // 2ï¸âƒ£ Create PeerConnection
        pc = new RTCPeerConnection();
        debugLog("RTCPeerConnection created");

        const dc = pc.createDataChannel("oai-debug");
        dc.onopen = () => debugLog("âœ… Data channel open with AI");
        dc.onmessage = (event) => debugLog("ðŸ’¬ AI event:", event.data);

        // 3ï¸âƒ£ Handle AI audio track
        pc.ontrack = event => {
          debugLog("ðŸ”¹ðŸ”¹ðŸ”¹ Received AI audio track");
          const audio = document.createElement("audio");
          audio.srcObject = event.streams[0];
          audio.autoplay = true;
          audio.controls = true;
          document.body.appendChild(audio);
        };

        pc.onconnectionstatechange = () => debugLog("connectionState:", pc.connectionState);
        pc.oniceconnectionstatechange = () => debugLog("iceConnectionState:", pc.iceConnectionState);
        pc.onicecandidate = e => { if (e.candidate) debugLog("New ICE candidate:", e.candidate); };

        // 4ï¸âƒ£ Capture microphone audio
        // const mic = await navigator.mediaDevices.getUserMedia({ audio: true });
        const mic = await navigator.mediaDevices.getUserMedia({
          audio: {
            noiseSuppression: true,
            echoCancellation: true,
          }
        });
        mic.getTracks().forEach(track => pc.addTrack(track, mic));
        debugLog("Microphone track added");

        // ðŸ”¹ DEBUG: check that tracks are attached to the PeerConnection
        pc.getSenders().forEach(sender => {
          console.log("[DEBUG] Sender track attached to PeerConnection:", sender.track);
        });

        // ðŸ”¹ Optional: log mic activity in real-time
        const audioContext = new AudioContext();
        const source = audioContext.createMediaStreamSource(mic);
        const analyser = audioContext.createAnalyser();
        source.connect(analyser);
        const dataArray = new Uint8Array(analyser.frequencyBinCount);

        function checkMicActivity() {
          analyser.getByteTimeDomainData(dataArray);
          const max = Math.max(...dataArray);
          if (max > 128) console.log("ðŸŽ¤ Mic is active");
          requestAnimationFrame(checkMicActivity);
        }
        checkMicActivity();

        // ðŸ”¹ Show speaking indicator as soon as mic is active
        showSpeakingIndicator();

        // Optional: hide after a while (or when session ends)
        // You could hide it when connectionState becomes "disconnected" or on track end:
        mic.getTracks().forEach(track => {
          track.onended = () => hideSpeakingIndicator();
        });

        // 5ï¸âƒ£ Create local SDP offer
        const offer = await pc.createOffer();
        await pc.setLocalDescription(offer);
        debugLog("Local SDP offer created");

        // 6ï¸âƒ£ Send offer to backend, backend will forward to OpenAI
        const resp = await fetch("/start-offer", {
          method: "POST",
          headers: { "Content-Type": "application/sdp" },
          body: offer.sdp
        });

        const answerSdp = await resp.text();
        debugLog("Received SDP answer from backend:", answerSdp);

        await pc.setRemoteDescription({ type: "answer", sdp: answerSdp });
        debugLog("Remote description set, connection should start");
      } catch (err) {
        console.error("[ERROR] Failed to start WebRTC session:", err);
      }
    }

    async function stopAndSend() {
      debugLog("Stop & Send triggered");

      // Stop all mic tracks
      if (micStream) {
        micStream.getTracks().forEach(track => track.stop());
      }

      hideSpeakingIndicator();

      if (!pc || !clientSecret) {
        debugLog("No active PeerConnection or client secret");
        return;
      }

      try {
        // Send local description (SDP) to backend manually
        const offer = pc.localDescription;
        if (!offer) {
          debugLog("No local SDP offer to send");
          return;
        }

        const response = await fetch("/start-offer", {
          method: "POST",
          headers: { "Content-Type": "application/sdp" },
          body: offer.sdp
        });
        debugLog("Manual offer sent to backend, status:", response.status);
      } catch (err) {
        console.error("[ERROR] Failed to send manual offer:", err);
      }
    }

    function showSpeakingIndicator() {
      const el = document.getElementById("speakingIndicator");
      if (el) el.style.display = "block";
    }

    function hideSpeakingIndicator() {
      const el = document.getElementById("speakingIndicator");
      if (el) el.style.display = "none";
    }

    // Bind button after DOM is loaded
    document.addEventListener("DOMContentLoaded", () => {
      const startBtn = document.getElementById("startBtn");
      startBtn.onclick = start;
      debugLog("Start button bound to start()");

      const stopBtn = document.getElementById("stopBtn");
      stopBtn.onclick = stopAndSend;
      debugLog("Stop button bound to stopAndSend()");
    });
  </script>
</body>
</html>
